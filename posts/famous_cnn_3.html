<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>Yufan's Blog - CV 常见 CNN 模型合集和基于 TensorFlow 的实现 - Part III</title>
    <!-- Icon -->
    <link rel="icon" type="image/png" href="/img/icon.png">
    <!-- Bootstrap core CSS -->
    <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <!-- Custom fonts for this template -->
    <link href="../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <!-- Rainbow css for the code -->
    <link href="../css/rainbow/monokai.css" rel="stylesheet" type="text/css">
    <!-- Custom styles for this template -->
    <link href="../css/clean-blog.min.css" rel="stylesheet">
  </head>
  <body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="/index.html">Yufan Zheng</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="/index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/about.html">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/blogs.html">Blogs</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/contact.html">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>
    <!-- Page Header -->
    <header class="masthead" style="background-image: url('../img/blogs/famous_cnn/bg.png')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>[CV] 常见 CNN 模型合集和基于 TensorFlow 的实现 - Part III</h1>
              <br>
              <h2 class="subheading">介绍 Inception 系列和 ResNet 模型，并附上简单易懂的基于 TensorFlow 的实现代码</h2>
              <span class="meta">Posted by <a href="/about.html">Yufan Zheng</a> on May 25, 2018</span>
            </div>
          </div>
        </div>
      </div>
    </header>
    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <p>上一篇博客介绍了 GoogLeNet， GoogLeNet 有一个另外的名字，叫做 Inception-v1，那么既然有 v1，肯定就会有一个系列紧随其后，这篇博客首先介绍一下 Inception 系列的进化过程，和另外一个牛逼哄哄的网络 ResNet。</p>
            <h2 class="section-heading"><a href="https://arxiv.org/abs/1502.03167">Inception-V2</a></h2>
            <p>不得不说，谷歌的团队就是厉害，在 GoogLeNet 发明后，不停的对它进行改良，发表了一个又一个更加厉害的网络。</p>
            <p>Inception-V2 是 GoogLeNet 的第一个改良版，与前一个版本相比，它的主要变化总结起来有两个：</p>
            <ul>
              <li>
                1. 加入了 BN （Batch Normalization）层，增加了这一层的好处有：
                <ul>
                  <li>1. 1  减少了 Internal Covariate Shift（内部 neuron 的数据分布发生变化），使每一层的输出都规范化到一个 N ( 0 , 1 ) 的高斯。下图展示了加入了 Batch Normalization 和没有加入 BN 层的产生的权值分布的变化图。</li>
                  <div class="text-center">
                    <img class="img-fluid" src="../img/blogs/famous_cnn/batch_norm.png">
                  </div>
                  <li>1. 2  增加了模型的鲁棒性，可以以更大的学习速率训练，收敛更快，初始化操作更加随意。</li>
                  <li>1. 3  作为一种正则化技术，可以减少 dropout 层的使用。</li>
                </ul>
              </li>
              <li>2. 用 2 个连续的 3 x 3 conv 替代 inception 模块中的 5 x 5，从而实现网络深度的增加，网络整体深度增加了 9 层，缺点就是增加了 25% 的 weights 和 30% 的计算消耗。</li>
              <div class="text-center">
                <img class="img-fluid" src="../img/blogs/famous_cnn/incp2">
              </div>
            </ul>
            <p>Inception-V2 的架构图如下</p>
            <div class="text-center">
              <img class="img-fluid" src="../img/blogs/famous_cnn/incep2.png">
            </div>
            <h2 class="section-heading"><a href="https://arxiv.org/abs/1512.00567">Inception-V3</a></h2>
            <p>Inception-V3 网络，主要在 V2 的基础上，提出了卷积分解（Factorization）的概念，为什么要提出这个概念呢？主要是为了在保持特征提取性能的同事，减少计算量。</p>
            <p>那么卷积分解（Factorization），具体是怎么做的呢？看下面两个例子。</p>
            <p>1. 对于 “ 大 ” 的卷积核，例如下图的 5 x 5 的卷积核，将其转换成两个 3 x 3 的卷积核，这样，需要计算的参数量就是之前的 18 / 25。</p>
            <div class="text-center">
              <img class="img-fluid" src="../img/blogs/famous_cnn/reduce1.png">
            </div>
            <p>2. 对于 “ 小 ” 的卷积核，例如下图的 3 x 3 的卷积核，将其转换成 1 x 3 和 3 x 1 的卷积核，这样，需要计算的参数量就是之前的 2 / 3。这里需要注意的是，为什么不用两个 2 x 2 的卷积核来分解呢？原因是两个 2 x 2 的卷积核只能减少 1 / 9 的计算量。而 1 x 3 却能减少 1 / 3 的计算量。</p>
            <div class="text-center">
              <img class="img-fluid" src="../img/blogs/famous_cnn/reduce2.png">
            </div>
            <p>作者提出，在实际运用中，太早的时候用 1 x 3 和 3 x 1 的卷积核效果不好，所以在网络的前期，往往采用 1 x 7 和 7 x 1 的尺度。</p>
            <p>好，回归正题，在 Inception-V3 中，卷积分解（Factorization）具体是怎么做的呢？其实在 V3 模型中，他们一共用到了 “ 三种 ” 不同的 Inception 的结构。对应在架构中的位置分别如下。</p>
            <div class="text-center">
              <img class="img-fluid" src="../img/blogs/famous_cnn/v31.png">
            </div>
            <p>=> 第一个 Inception 和 V2 中的一样，只是将 V1 版本中的 5 x 5 卷积替换成了两个 3 x 3</p>
            <p>=> 第二个 Inception 进行了横向和纵向的因式分解，将 n x n 分解成了 n x 1 和 1 x n。</p>
            <p>=> 第三个 Inception 将 3 x 3 的卷积核进行了再次拆解。</p>
            <p>为了进一步的减少计算量，在 Inception 层与层之间的结构，也进行了网格大小的优化。卷积网络通过池化层来减少特征图的 Grid Size，而交换卷积层和池化层的位置，可以减少计算量。例如：</p>
            <div class="text-center">
              <img class="img-fluid" src="../img/blogs/famous_cnn/size_reduce1.png">
            </div>
            <p>Inception V3 中对比了两种模型，他们的区别是，到底是先做 Inception 再做 Pooling 呢？还是先做 Pooling 再做 Inception 呢？</p>
            <div class="text-center">
              <img class="img-fluid" src="../img/blogs/famous_cnn/fig9.png">
            </div>
            <p>左边的做法，不满足 representional bottleneck 原则，即是，在网络的前期，不宜下降的太快。右边虽然满足了这个原则，但是计算量却是左边的 3 倍。</p>
            <p>为了解决这个冲突，Inception-V3 提出了一个能同时满足两个优势的模型，如下图。</p>
            <div class="text-center">
              <img class="img-fluid" src="../img/blogs/famous_cnn/fig10.png">
            </div>
            <p>那么，加上这个层与层之间的连接模块，Inception-V3 的最终版本终于浮出了水面，它的最终结构如下图。 </p>
            <div class="text-center">
              <img class="img-fluid" src="../img/blogs/famous_cnn/v32.png">
            </div>
            <p>注意到，Inception-V3 相比于 GoogLeNet，层数达到了 42 层，是原来的 2.5 倍，已经深到作者都无法在 paper 中画出来了，只能靠粗略的简图概略结构了。（GoogLeNet 的论文中是有它的完整层次结构的。）</p>
            <h2 class="section-heading"><a href="https://arxiv.org/abs/1512.03385">ResNet</a></h2>
            <p>这里先不介绍 Inception-V4， 因为要理解 Inception-v4 的话，必须要先弄明白 ResNet 的概念，因为实际上 Inception-V4 主要就是看到ResNet觉得挺好，就拿来和 Inception 模块结合一下，加 shortcut。</p>
            <div>
              <div class="RichText ztext Post-RichText">
                <p><b>欢迎交流与转载，文章会同步发布在公众号：机器学习算法全栈工程师(Jeemy110)</b></p>
                <h2>引言</h2>
                <p>深度残差网络（Deep residual network, ResNet）的提出是CNN图像史上的一件里程碑事件，让我们先看一下ResNet在ILSVRC和COCO 2015上的战绩：</p>
                <figure>
                  <noscript><img src="https://pic1.zhimg.com/v2-5e98ec97def099a4e6fb6b7ce3b1d460_b.jpg" data-size="normal" data-rawwidth="534" data-rawheight="233" class="origin_image zh-lightbox-thumb" width="534" data-original="https://pic1.zhimg.com/v2-5e98ec97def099a4e6fb6b7ce3b1d460_r.jpg"></noscript>
                  <img src="https://pic1.zhimg.com/80/v2-5e98ec97def099a4e6fb6b7ce3b1d460_hd.jpg" data-size="normal" data-rawwidth="534" data-rawheight="233" class="origin_image zh-lightbox-thumb lazy" width="534" data-original="https://pic1.zhimg.com/v2-5e98ec97def099a4e6fb6b7ce3b1d460_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-5e98ec97def099a4e6fb6b7ce3b1d460_b.jpg">
                  <figcaption>图1 ResNet在ILSVRC和COCO 2015上的战绩</figcaption>
                </figure>
                <p>ResNet取得了5项第一，并又一次刷新了CNN模型在ImageNet上的历史：</p>
                <figure>
                  <noscript><img src="https://pic1.zhimg.com/v2-606573bdaaa97de6b8b10fb00f76d29a_b.jpg" data-size="normal" data-rawwidth="486" data-rawheight="260" class="origin_image zh-lightbox-thumb" width="486" data-original="https://pic1.zhimg.com/v2-606573bdaaa97de6b8b10fb00f76d29a_r.jpg"></noscript>
                  <img src="https://pic1.zhimg.com/80/v2-606573bdaaa97de6b8b10fb00f76d29a_hd.jpg" data-size="normal" data-rawwidth="486" data-rawheight="260" class="origin_image zh-lightbox-thumb lazy" width="486" data-original="https://pic1.zhimg.com/v2-606573bdaaa97de6b8b10fb00f76d29a_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-606573bdaaa97de6b8b10fb00f76d29a_b.jpg">
                  <figcaption>图2 ImageNet分类Top-5误差</figcaption>
                </figure>
                <p>ResNet的作者<a href="https://link.zhihu.com/?target=http%3A//kaiminghe.com/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">何凯明</a>也因此摘得CVPR2016最佳论文奖，当然何博士的成就远不止于此，感兴趣的可以去搜一下他后来的辉煌战绩。那么ResNet为什么会有如此优异的表现呢？其实ResNet是解决了深度CNN模型难训练的问题，从图2中可以看到14年的VGG才19层，而15年的ResNet多达152层，这在网络深度完全不是一个量级上，所以如果是第一眼看这个图的话，肯定会觉得ResNet是靠深度取胜。事实当然是这样，但是ResNet还有架构上的trick，这才使得网络的深度发挥出作用，这个trick就是残差学习（Residual learning）。下面详细讲述ResNet的理论及实现。</p>
                <h2>深度网络的退化问题</h2>
                <p>从经验来看，网络的深度对模型的性能至关重要，当增加网络层数后，网络可以进行更加复杂的特征模式的提取，所以当模型更深时理论上可以取得更好的结果，从图2中也可以看出网络越深而效果越好的一个实践证据。但是更深的网络其性能一定会更好吗？实验发现深度网络出现了退化问题（Degradation problem）：网络深度增加时，网络准确度出现饱和，甚至出现下降。这个现象可以在图3中直观看出来：56层的网络比20层网络效果还要差。这不会是过拟合问题，因为56层网络的训练误差同样高。我们知道深层网络存在着梯度消失或者爆炸的问题，这使得深度学习模型很难训练。但是现在已经存在一些技术手段如BatchNorm来缓解这个问题。因此，出现深度网络的退化问题是非常令人诧异的。</p>
                <figure>
                  <noscript><img src="https://pic3.zhimg.com/v2-dcf5688dad675cbe8fb8be243af5e1fd_b.jpg" data-size="normal" data-rawwidth="602" data-rawheight="197" class="origin_image zh-lightbox-thumb" width="602" data-original="https://pic3.zhimg.com/v2-dcf5688dad675cbe8fb8be243af5e1fd_r.jpg"></noscript>
                  <img src="https://pic3.zhimg.com/80/v2-dcf5688dad675cbe8fb8be243af5e1fd_hd.jpg" data-size="normal" data-rawwidth="602" data-rawheight="197" class="origin_image zh-lightbox-thumb lazy" width="602" data-original="https://pic3.zhimg.com/v2-dcf5688dad675cbe8fb8be243af5e1fd_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-dcf5688dad675cbe8fb8be243af5e1fd_b.jpg">
                  <figcaption>图3 20层与56层网络在CIFAR-10上的误差</figcaption>
                </figure>
                <h2>残差学习</h2>
                <p>深度网络的退化问题至少说明深度网络不容易训练。但是我们考虑这样一个事实：现在你有一个浅层网络，你想通过向上堆积新层来建立深层网络，一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即这样新层是恒等映射（Identity mapping）。在这种情况下，深层网络应该至少和浅层网络性能一样，也不应该出现退化现象。好吧，你不得不承认肯定是目前的训练方法有问题，才使得深层网络很难去找到一个好的参数。</p>
                <p>这个有趣的假设让何博士灵感爆发，他提出了残差学习来解决退化问题。对于一个堆积层结构（几层堆积而成）当输入为 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"> 时其学习到的特征记为 <img src="https://www.zhihu.com/equation?tex=H%28x%29" alt="H(x)" eeimg="1"> ，现在我们希望其可以学习到残差 <img src="https://www.zhihu.com/equation?tex=F%28x%29%3DH%28x%29-x" alt="F(x)=H(x)-x" eeimg="1"> ，这样其实原始的学习特征是 <img src="https://www.zhihu.com/equation?tex=F%28x%29%2Bx" alt="F(x)+x" eeimg="1"> 。之所以这样是因为残差学习相比原始特征直接学习更容易。当残差为0时，此时堆积层仅仅做了恒等映射，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。残差学习的结构如图4所示。这有点类似与电路中的“短路”，所以是一种短路连接（shortcut connection）。</p>
                <figure>
                  <noscript><img src="https://pic4.zhimg.com/v2-252e6d9979a2a91c2d3033b9b73eb69f_b.jpg" data-size="normal" data-rawwidth="407" data-rawheight="219" class="content_image" width="407"></noscript>
                  <img src="https://pic4.zhimg.com/80/v2-252e6d9979a2a91c2d3033b9b73eb69f_hd.jpg" data-size="normal" data-rawwidth="407" data-rawheight="219" class="content_image lazy" width="407" data-actualsrc="https://pic4.zhimg.com/v2-252e6d9979a2a91c2d3033b9b73eb69f_b.jpg">
                  <figcaption>图4 残差学习单元</figcaption>
                </figure>
                <p>为什么残差学习相对更容易，从直观上看残差学习需要学习的内容少，因为残差一般会比较小，学习难度小点。不过我们可以从数学的角度来分析这个问题，首先残差单元可以表示为：</p>
                <p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26+%7B%7By%7D_%7Bl%7D%7D%3Dh%28%7B%7Bx%7D_%7Bl%7D%7D%29%2BF%28%7B%7Bx%7D_%7Bl%7D%7D%2C%7B%7BW%7D_%7Bl%7D%7D%29+%5C%5C+%26+%7B%7Bx%7D_%7Bl%2B1%7D%7D%3Df%28%7B%7By%7D_%7Bl%7D%7D%29+%5C%5C+%5Cend%7Balign%7D+" alt="\begin{align} &amp; {{y}_{l}}=h({{x}_{l}})+F({{x}_{l}},{{W}_{l}}) \\ &amp; {{x}_{l+1}}=f({{y}_{l}}) \\ \end{align} " eeimg="1"> </p>
                <p>其中 <img src="https://www.zhihu.com/equation?tex=x_%7Bl%7D" alt="x_{l}" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D" alt="x_{l+1}" eeimg="1"> 分别表示的是第 <img src="https://www.zhihu.com/equation?tex=l" alt="l" eeimg="1"> 个残差单元的输入和输出，注意每个残差单元一般包含多层结构。 <img src="https://www.zhihu.com/equation?tex=F" alt="F" eeimg="1"> 是残差函数，表示学习到的残差，而 <img src="https://www.zhihu.com/equation?tex=h%28x_%7Bl%7D%29%3Dx_%7Bl%7D" alt="h(x_{l})=x_{l}" eeimg="1"> 表示恒等映射， <img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1"> 是ReLU激活函数。基于上式，我们求得从浅层 <img src="https://www.zhihu.com/equation?tex=l" alt="l" eeimg="1"> 到深层 <img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 的学习特征为：</p>
                <p><img src="https://www.zhihu.com/equation?tex=%7B%7Bx%7D_%7BL%7D%7D%3D%7B%7Bx%7D_%7Bl%7D%7D%2B%5Csum%5Climits_%7Bi%3Dl%7D%5E%7BL-1%7D%7BF%28%7B%7Bx%7D_%7Bi%7D%7D%7D%2C%7B%7BW%7D_%7Bi%7D%7D%29" alt="{{x}_{L}}={{x}_{l}}+\sum\limits_{i=l}^{L-1}{F({{x}_{i}}},{{W}_{i}})" eeimg="1"> </p>
                <p>利用链式规则，可以求得反向过程的梯度：</p>
                <p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7Bl%7D%7D%7D%3D%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D%5Ccdot+%5Cfrac%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D%7B%5Cpartial+%7B%7Bx%7D_%7Bl%7D%7D%7D%3D%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D%5Ccdot+%5Cleft%28+1%2B%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D%5Csum%5Climits_%7Bi%3Dl%7D%5E%7BL-1%7D%7BF%28%7B%7Bx%7D_%7Bi%7D%7D%2C%7B%7BW%7D_%7Bi%7D%7D%29%7D+%5Cright%29" alt="\frac{\partial loss}{\partial {{x}_{l}}}=\frac{\partial loss}{\partial {{x}_{L}}}\cdot \frac{\partial {{x}_{L}}}{\partial {{x}_{l}}}=\frac{\partial loss}{\partial {{x}_{L}}}\cdot \left( 1+\frac{\partial }{\partial {{x}_{L}}}\sum\limits_{i=l}^{L-1}{F({{x}_{i}},{{W}_{i}})} \right)" eeimg="1"> </p>
                <p>式子的第一个因子 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D" alt="\frac{\partial loss}{\partial {{x}_{L}}}" eeimg="1"> 表示的损失函数到达 <img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。要注意上面的推导并不是严格的证明。</p>
                <h2>ResNet的网络结构</h2>
                <p>ResNet网络是参考了VGG19网络，在其基础上进行了修改，并通过短路机制加入了残差单元，如图5所示。变化主要体现在ResNet直接使用stride=2的卷积做下采样，并且用global average pool层替换了全连接层。ResNet的一个重要设计原则是：当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。从图5中可以看到，ResNet相比普通网络每两层间增加了短路机制，这就形成了残差学习，其中虚线表示feature map数量发生了改变。图5展示的34-layer的ResNet，还可以构建更深的网络如表1所示。从表中可以看到，对于18-layer和34-layer的ResNet，其进行的两层间的残差学习，当网络更深时，其进行的是三层间的残差学习，三层卷积核分别是1x1，3x3和1x1，一个值得注意的是隐含层的feature map数量是比较小的，并且是输出feature map数量的1/4。</p>
                <figure>
                  <noscript><img src="https://pic3.zhimg.com/v2-7cb9c03871ab1faa7ca23199ac403bd9_b.jpg" data-size="normal" data-rawwidth="469" data-rawheight="1077" class="origin_image zh-lightbox-thumb" width="469" data-original="https://pic3.zhimg.com/v2-7cb9c03871ab1faa7ca23199ac403bd9_r.jpg"></noscript>
                  <img src="https://pic3.zhimg.com/80/v2-7cb9c03871ab1faa7ca23199ac403bd9_hd.jpg" data-size="normal" data-rawwidth="469" data-rawheight="1077" class="origin_image zh-lightbox-thumb lazy" width="469" data-original="https://pic3.zhimg.com/v2-7cb9c03871ab1faa7ca23199ac403bd9_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-7cb9c03871ab1faa7ca23199ac403bd9_b.jpg">
                  <figcaption>图5 ResNet网络结构图</figcaption>
                </figure>
                <figure>
                  <noscript><img src="https://pic4.zhimg.com/v2-1dfd4022d4be28392ff44c49d6b4ed94_b.jpg" data-size="normal" data-rawwidth="554" data-rawheight="242" class="origin_image zh-lightbox-thumb" width="554" data-original="https://pic4.zhimg.com/v2-1dfd4022d4be28392ff44c49d6b4ed94_r.jpg"></noscript>
                  <img src="https://pic4.zhimg.com/80/v2-1dfd4022d4be28392ff44c49d6b4ed94_hd.jpg" data-size="normal" data-rawwidth="554" data-rawheight="242" class="origin_image zh-lightbox-thumb lazy" width="554" data-original="https://pic4.zhimg.com/v2-1dfd4022d4be28392ff44c49d6b4ed94_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-1dfd4022d4be28392ff44c49d6b4ed94_b.jpg">
                  <figcaption>表1 不同深度的ResNet</figcaption>
                </figure>
                <p>下面我们再分析一下残差单元，ResNet使用两种残差单元，如图6所示。左图对应的是浅层网络，而右图对应的是深层网络。对于短路连接，当输入和输出维度一致时，可以直接将输入加到输出上。但是当维度不一致时（对应的是维度增加一倍），这就不能直接相加。有两种策略：（1）采用zero-padding增加维度，此时一般要先做一个downsamp，可以采用strde=2的pooling，这样不会增加参数；（2）采用新的映射（projection shortcut），一般采用1x1的卷积，这样会增加参数，也会增加计算量。短路连接除了直接使用恒等映射，当然都可以采用projection shortcut。</p>
                <figure>
                  <noscript><img src="https://pic3.zhimg.com/v2-0892e5423616c30f69ded61111b111c0_b.jpg" data-size="normal" data-rawwidth="566" data-rawheight="203" class="origin_image zh-lightbox-thumb" width="566" data-original="https://pic3.zhimg.com/v2-0892e5423616c30f69ded61111b111c0_r.jpg"></noscript>
                  <img src="https://pic3.zhimg.com/80/v2-0892e5423616c30f69ded61111b111c0_hd.jpg" data-size="normal" data-rawwidth="566" data-rawheight="203" class="origin_image zh-lightbox-thumb lazy" width="566" data-original="https://pic3.zhimg.com/v2-0892e5423616c30f69ded61111b111c0_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-0892e5423616c30f69ded61111b111c0_b.jpg">
                  <figcaption>图6 不同的残差单元</figcaption>
                </figure>
                <p>作者对比18-layer和34-layer的网络效果，如图7所示。可以看到普通的网络出现退化现象，但是ResNet很好的解决了退化问题。</p>
                <figure>
                  <noscript><img src="https://pic4.zhimg.com/v2-ac88d9e118e3a85922188daba84f7efd_b.jpg" data-size="normal" data-rawwidth="710" data-rawheight="220" class="origin_image zh-lightbox-thumb" width="710" data-original="https://pic4.zhimg.com/v2-ac88d9e118e3a85922188daba84f7efd_r.jpg"></noscript>
                  <img src="https://pic4.zhimg.com/80/v2-ac88d9e118e3a85922188daba84f7efd_hd.jpg" data-size="normal" data-rawwidth="710" data-rawheight="220" class="origin_image zh-lightbox-thumb lazy" width="710" data-original="https://pic4.zhimg.com/v2-ac88d9e118e3a85922188daba84f7efd_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-ac88d9e118e3a85922188daba84f7efd_b.jpg">
                  <figcaption>图7 18-layer和34-layer的网络效果</figcaption>
                </figure>
                <p>最后展示一下ResNet网络与其他网络在ImageNet上的对比结果，如表2所示。可以看到ResNet-152其误差降到了4.49%，当采用集成模型后，误差可以降到3.57%。</p>
                <figure>
                  <noscript><img src="https://pic3.zhimg.com/v2-0a2c8a209a221817f91c1f1728327beb_b.jpg" data-size="normal" data-rawwidth="528" data-rawheight="330" class="origin_image zh-lightbox-thumb" width="528" data-original="https://pic3.zhimg.com/v2-0a2c8a209a221817f91c1f1728327beb_r.jpg"></noscript>
                  <img src="https://pic3.zhimg.com/80/v2-0a2c8a209a221817f91c1f1728327beb_hd.jpg" data-size="normal" data-rawwidth="528" data-rawheight="330" class="origin_image zh-lightbox-thumb lazy" width="528" data-original="https://pic3.zhimg.com/v2-0a2c8a209a221817f91c1f1728327beb_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-0a2c8a209a221817f91c1f1728327beb_b.jpg">
                  <figcaption>表2 ResNet与其他网络的对比结果</figcaption>
                </figure>
                <p>说一点关于残差单元题外话，上面我们说到了短路连接的几种处理方式，其实作者在<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.05027" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">文献[2]</a>中又对不同的残差单元做了细致的分析与实验，这里我们直接抛出最优的残差结构，如图8所示。改进前后一个明显的变化是采用pre-activation，BN和ReLU都提前了。而且作者推荐短路连接采用恒等变换，这样保证短路连接不会有阻碍。感兴趣的可以去读读这篇文章。</p>
                <figure>
                  <noscript><img src="https://pic4.zhimg.com/v2-4e0bf37ecad2f306fe09d32a2d37d908_b.jpg" data-size="normal" data-rawwidth="729" data-rawheight="377" class="origin_image zh-lightbox-thumb" width="729" data-original="https://pic4.zhimg.com/v2-4e0bf37ecad2f306fe09d32a2d37d908_r.jpg"></noscript>
                  <img src="https://pic4.zhimg.com/80/v2-4e0bf37ecad2f306fe09d32a2d37d908_hd.jpg" data-size="normal" data-rawwidth="729" data-rawheight="377" class="origin_image zh-lightbox-thumb lazy" width="729" data-original="https://pic4.zhimg.com/v2-4e0bf37ecad2f306fe09d32a2d37d908_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-4e0bf37ecad2f306fe09d32a2d37d908_b.jpg">
                  <figcaption>图8 改进后的残差单元及效果</figcaption>
                </figure>
                <h2>ResNet的TensorFlow实现</h2>
                <p>这里给出ResNet50的TensorFlow实现，模型的实现参考了<a href="https://link.zhihu.com/?target=https%3A//github.com/KaimingHe/deep-residual-networks" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Caffe版本</a>的实现，核心代码如下：</p>
                <p>完整实现可以参见<a href="https://link.zhihu.com/?target=https%3A//github.com/xiaohu2015/DeepLearning_tutorials/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">GitHub</a>。</p>
                <h2>总结 </h2>
                <p>ResNet通过残差学习解决了深度网络的退化问题，让我们可以训练出更深的网络，这称得上是深度网络的一个历史大突破吧。也许不久会有更好的方式来训练更深的网络，让我们一起期待吧！</p>
                <h2>参考资料</h2>
                <ol>
                  <li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1512.03385" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Deep Residual Learning for Image Recognition</a>.</li>
                  <li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.05027" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Identity Mappings in Deep Residual Networks</a>.</li>
                  <li><a href="https://link.zhihu.com/?target=http%3A//kaiminghe.com/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">去膜拜一下大神</a>.</li>
                </ol>
                <p><br><b>欢迎交流与转载，文章会同步发布在公众号：机器学习算法全栈工程师(Jeemy110)</b></p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </article>
    <hr>
    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="#">
                <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="#">
                <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="https://github.com/alphafan">
                <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
                </a>
              </li>
            </ul>
            <p class="copyright text-muted">Copyright &copy; Yufan's Website 2017</p>
          </div>
        </div>
      </div>
    </footer>
    <!-- Bootstrap core JavaScript -->
    <script src="../vendor/jquery/jquery.min.js"></script>
    <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <!-- Custom scripts for this template -->
    <script src="../js/clean-blog.min.js"></script>
    <!-- Rainbow Python -->
    <script src="../js/rainbow.min.js"></script>
    <script src="../js/rainbow.python.js"></script>
  </body>
</html>
</html>